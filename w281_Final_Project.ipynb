{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W281 Final Project: Intel Image Classification Model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import gc\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout,BatchNormalization,MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a16RGOC4-xjw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zdbrown13/w281/w281_Final_Project_Brown_Benzoni_Olaya\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(directory, num_files=5):\n",
    "    images = []\n",
    "    file_list = os.listdir(directory)[:num_files]  # Load only the first 5 files\n",
    "\n",
    "    for filename in tqdm(file_list, desc=f\"Loading images from {directory}\"):\n",
    "        img_path = os.path.join(directory, filename)\n",
    "        img = Image.open(img_path)\n",
    "        img = img.resize((150, 150))  # Resize image to 150 x 150\n",
    "        images.append(img)\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Paths\n",
    "\n",
    "buildings_train = './seg_train/buildings'\n",
    "forest_train = './seg_train/forest'\n",
    "glacier_train = './seg_train/glacier'\n",
    "mountain_train = './seg_train/mountain'\n",
    "sea_train = './seg_train/sea'\n",
    "street_train = './seg_train/street'\n",
    "\n",
    "buildings_test = './seg_test/buildings'\n",
    "forest_test = './seg_test/forest'\n",
    "glacier_test = './seg_test/glacier'\n",
    "mountain_test = './seg_test/mountain'\n",
    "sea_test = './seg_test/sea'\n",
    "street_test = './seg_test/street'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images from ./seg_train/buildings: 100%|██████████| 5/5 [00:00<00:00, 496.19it/s]\n",
      "Loading images from ./seg_train/forest: 100%|██████████| 5/5 [00:00<00:00, 989.69it/s]\n",
      "Loading images from ./seg_train/glacier: 100%|██████████| 5/5 [00:00<00:00, 1132.00it/s]\n",
      "Loading images from ./seg_train/mountain: 100%|██████████| 5/5 [00:00<00:00, 1510.59it/s]\n",
      "Loading images from ./seg_train/sea: 100%|██████████| 5/5 [00:00<00:00, 1466.33it/s]\n",
      "Loading images from ./seg_train/street: 100%|██████████| 5/5 [00:00<00:00, 1187.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load images from each category\n",
    "buildings_img = load_images(buildings_train)\n",
    "forest_img = load_images(forest_train)\n",
    "glacier_img = load_images(glacier_train)\n",
    "mountain_img = load_images(mountain_train)\n",
    "sea_img = load_images(sea_train)\n",
    "street_img = load_images(street_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.Image.Image image mode=RGB size=150x150>,\n",
       " <PIL.Image.Image image mode=RGB size=150x150>,\n",
       " <PIL.Image.Image image mode=RGB size=150x150>,\n",
       " <PIL.Image.Image image mode=RGB size=150x150>,\n",
       " <PIL.Image.Image image mode=RGB size=150x150>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buildings_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datasets):\n",
    "    \n",
    "    output = []\n",
    "\n",
    "    for dataset in datasets:\n",
    "        images, labels = [], []\n",
    "        print(f\"Loading {dataset}...\")\n",
    "        i = 0\n",
    "        for folder in os.listdir(dataset):\n",
    "            label = i # Converting word labels to int (i.e. buildings = 0)\n",
    "            i = i+1\n",
    "            folder_path = os.path.join(dataset, folder)\n",
    "\n",
    "            for file in tqdm(os.listdir(folder_path), desc=f\"Processing {folder}\"):\n",
    "                img_path = os.path.join(folder_path, file)\n",
    "\n",
    "                image = Image.open(img_path).resize((150, 150))\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "                \n",
    "        images = np.stack(images)\n",
    "        labels = np.array(labels, dtype='int32')\n",
    "        output.append((images, labels))\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"/Users/zdbrown13/w281/w281_Final_Project_Brown_Benzoni_Olaya/seg_train\", \n",
    "        \"/Users/zdbrown13/w281/w281_Final_Project_Brown_Benzoni_Olaya/seg_test\"]\n",
    "\n",
    "\n",
    "train_path, test_path = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /Users/zdbrown13/w281/w281_Final_Project_Brown_Benzoni_Olaya/seg_train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing forest: 100%|██████████| 2271/2271 [00:01<00:00, 1538.09it/s]\n",
      "Processing buildings: 100%|██████████| 2191/2191 [00:01<00:00, 1788.68it/s]\n",
      "Processing glacier: 100%|██████████| 2404/2404 [00:01<00:00, 1973.56it/s]\n",
      "Processing street: 100%|██████████| 2382/2382 [00:01<00:00, 1856.72it/s]\n",
      "Processing mountain: 100%|██████████| 2512/2512 [00:01<00:00, 2082.61it/s]\n",
      "Processing sea: 100%|██████████| 2274/2274 [00:01<00:00, 2115.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /Users/zdbrown13/w281/w281_Final_Project_Brown_Benzoni_Olaya/seg_test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing forest: 100%|██████████| 474/474 [00:00<00:00, 1606.70it/s]\n",
      "Processing buildings: 100%|██████████| 437/437 [00:00<00:00, 1940.83it/s]\n",
      "Processing glacier: 100%|██████████| 553/553 [00:00<00:00, 2027.59it/s]\n",
      "Processing street: 100%|██████████| 501/501 [00:00<00:00, 1928.89it/s]\n",
      "Processing mountain: 100%|██████████| 525/525 [00:00<00:00, 2148.51it/s]\n",
      "Processing sea: 100%|██████████| 510/510 [00:00<00:00, 2146.29it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = load_data(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "train_images = train_images / 255.0 \n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2271, 1: 2191, 2: 2404, 3: 2382, 4: 2512, 5: 2274}\n"
     ]
    }
   ],
   "source": [
    "# Training Data Distribution\n",
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2339.0\n"
     ]
    }
   ],
   "source": [
    "avg = sum(counts)/len(unique)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 474, 1: 437, 2: 553, 3: 501, 4: 525, 5: 510}\n"
     ]
    }
   ],
   "source": [
    "# Test Data Distribution\n",
    "test_unique, test_counts = np.unique(test_labels, return_counts=True)\n",
    "print(dict(zip(test_unique, test_counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0\n"
     ]
    }
   ],
   "source": [
    "test_avg = sum(test_counts)/len(test_unique)\n",
    "print(test_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest & SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the data for Random Forest & SVM\n",
    "X_train_flat = train_images.reshape(len(train_images), -1)\n",
    "X_test_flat = test_images.reshape(len(test_images), -1)\n",
    "y_train_trad = train_labels\n",
    "y_test_trad = test_labels\n",
    "\n",
    "# Create train/validation splits\n",
    "X_train_ml, X_val_ml, y_train_ml, y_val_ml = train_test_split(\n",
    "    X_train_flat,\n",
    "    y_train_trad,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest Classifier...\n",
      "\n",
      "Random Forest Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77       462\n",
      "           1       0.53      0.38      0.45       460\n",
      "           2       0.57      0.62      0.60       455\n",
      "           3       0.66      0.66      0.66       484\n",
      "           4       0.58      0.64      0.61       538\n",
      "           5       0.49      0.45      0.46       408\n",
      "\n",
      "    accuracy                           0.60      2807\n",
      "   macro avg       0.59      0.60      0.59      2807\n",
      "weighted avg       0.59      0.60      0.59      2807\n",
      "\n",
      "Validation Accuracy: 0.601\n"
     ]
    }
   ],
   "source": [
    "# RandomForest Model\n",
    "print(\"Training Random Forest Classifier...\")\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_ml, y_train_ml)\n",
    "\n",
    "# Evaluate Random Forest on validation set\n",
    "rf_val_pred = rf.predict(X_val_ml)\n",
    "print(\"\\nRandom Forest Validation Results:\")\n",
    "print(classification_report(y_val_ml, rf_val_pred))\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val_ml, rf_val_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SVM classifier...\n",
      "Starting SVM grid search...\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    }
   ],
   "source": [
    "def train_svm_classifier(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Train SVM classifier with hyperparameter tuning\n",
    "    \"\"\"\n",
    "    # Free memory before starting\n",
    "    gc.collect()\n",
    "\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1.0],  # Regularization parameter\n",
    "        'kernel': ['rbf'],  # Radial Basis Function kernel\n",
    "        'gamma': ['scale', 'auto']  # Kernel coefficient\n",
    "    }\n",
    "    \n",
    "    # Initialize SVM, specifying 2GB cache size for M1 chip.\n",
    "    svm = SVC(random_state=42, cache_size=2000)\n",
    "    \n",
    "    # Perform grid search\n",
    "    print(\"Starting SVM grid search...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        svm,\n",
    "        param_grid,\n",
    "        cv=2,  # 2-fold cross-validation\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nBest parameters found:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(\"\\nBest cross-validation accuracy: {:.3f}\".format(grid_search.best_score_))\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_accuracy = grid_search.score(X_val, y_val)\n",
    "    print(\"Validation accuracy: {:.3f}\".format(val_accuracy))\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "print(\"\\nTraining SVM classifier...\")\n",
    "svm_classifier = train_svm_classifier(X_train_ml, y_train_ml, X_val_ml, y_val_ml)\n",
    "\n",
    "# Evaluate SVM on test set\n",
    "svm_test_pred = svm_classifier.predict(X_test_flat)\n",
    "print(\"\\nSVM Test Results:\")\n",
    "print(classification_report(y_test_trad, svm_test_pred))\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test_trad, svm_test_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN & ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding for CNN/ResNet\n",
    "num_classes = len(np.unique(train_labels))\n",
    "train_labels_onehot = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels_onehot = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "# Create train/validation split for CNN/ResNet\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_images, \n",
    "    train_labels_onehot,\n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CNN Model\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Compile the CNN model\n",
    "cnn_model = create_cnn_model((150, 150, 3), num_classes)\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN Model\n",
    "cnn_history = cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "# Evaluate the CNN Model\n",
    "cnn_loss, cnn_accuracy = cnn_model.evaluate(test_images, test_labels_onehot)\n",
    "print(f\"CNN Model Accuracy: {cnn_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet50 Transfer Learning Model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "resnet_model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the ResNet model\n",
    "resnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the ResNet Model\n",
    "resnet_history = resnet_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "# Evaluate the ResNet Model\n",
    "resnet_loss, resnet_accuracy = resnet_model.evaluate(test_images, test_labels_onehot)\n",
    "print(f\"ResNet50 Model Accuracy: {resnet_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest vs. SVM vs. CNN vs. ResNet Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of Model Performances\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(f\"Random Forest Model Accuracy: {accuracy_score(y_test_rf, y_pred_rf) * 100:.2f}%\")\n",
    "print(f\"CNN Model Accuracy: {cnn_accuracy * 100:.2f}%\")\n",
    "print(f\"ResNet50 Model Accuracy: {resnet_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
